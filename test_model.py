# test_model.py
"""
LocalMind - EXAONE ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def test_localmind_model():
    try:
        print("ğŸ” LocalMind - EXAONE ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤...")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
        model_name = "LGAI-EXAONE/EXAONE-4.0-1.2B"
        
        print("   í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        print("   ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            trust_remote_code=True
        )
        
        # ì±„íŒ… í…œí”Œë¦¿ ì‚¬ìš©
        messages = [
            {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”."}
        ]
        
        # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        print(f"   ì‚¬ìš©ëœ í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt, return_tensors="pt")
        if device == "cuda":
            inputs = inputs.to(device)
        
        # ìƒì„±
        print("ğŸ“ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤...")
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        print(f"âœ… ì‘ë‹µ: {response}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_localmind_model()