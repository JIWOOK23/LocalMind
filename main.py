# main.py
"""
FAISS ê¸°ë°˜ ë¡œì»¬ RAG ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ë‚´ìš© ê¸°ë°˜ ì§ˆì˜ì‘ë‹µê³¼ ë¬¸ì²´ ëª¨ë°© ìƒì„± ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import pickle
import numpy as np
import faiss
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import warnings

# FutureWarning ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore", category=FutureWarning, module="torch")

class LocalMindSystem:
    """LocalMind - ë¡œì»¬ AI ë¬¸ì„œ ì–´ì‹œìŠ¤í„´íŠ¸ ì‹œìŠ¤í…œ í´ë˜ìŠ¤"""
    
    def __init__(self, faiss_path="document_memory.faiss", data_path="document_memory.pkl"):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.faiss_path = faiss_path
        self.data_path = data_path
        self.model = None
        self.tokenizer = None
        self.embedding_model = None
        self.index = None
        self.texts = []
        self.metadatas = []
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        self._initialize_system()
    
    def _initialize_system(self):
        """ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        try:
            print("ğŸš€ LocalMind ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
            
            # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(self.faiss_path) or not os.path.exists(self.data_path):
                print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print("   ë¨¼ì € 'python create_db.py'ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
                return
            
            # LLM ì´ˆê¸°í™”
            print("1. LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤... (EXAONE-4.0-1.2B)")
            print(f"   ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
            
            model_name = "LGAI-EXAONE/EXAONE-4.0-1.2B"
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True
            )
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True
            )
            
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            print("2. ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
            self.embedding_model = HuggingFaceEmbedding(model_name="BAAI/bge-m3")
            
            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            print("3. FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
            self.index = faiss.read_index(self.faiss_path)
            
            # í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
            print("4. í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
            with open(self.data_path, "rb") as f:
                data = pickle.load(f)
                self.texts = data["texts"]
                self.metadatas = data["metadatas"]
            
            print(f"   âœ… {len(self.texts)}ê°œì˜ ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            print(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            print("   - ì¸í„°ë„· ì—°ê²°ì´ ì•ˆì •ì ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš” (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)")
            print("   - GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš” (ìµœì†Œ 4GB ê¶Œì¥)")
            print("   - HuggingFace í† í°ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            import traceback
            traceback.print_exc()
    
    def _get_relevant_context(self, query, top_k=5):
        """ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            if not self.embedding_model or not self.index:
                return ""
            
            # ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
            query_embedding = self.embedding_model.get_text_embedding(query)
            query_vector = np.array([query_embedding]).astype('float32')
            
            # FAISSë¡œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
            scores, indices = self.index.search(query_vector, top_k)
            
            # ê´€ë ¨ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            context_parts = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.texts) and score > 0.1:  # ìœ ì‚¬ë„ ì„ê³„ê°’
                    text = self.texts[idx]
                    metadata = self.metadatas[idx]
                    context_parts.append(f"[ì¶œì²˜: {metadata['source']}] {text}")
            
            return "\n\n".join(context_parts)
            
        except Exception as e:
            print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return ""
    
    def _generate_response(self, prompt):
        """EXAONE ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±"""
        try:
            if not self.model or not self.tokenizer:
                return "âŒ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
            # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
            messages = [{"role": "user", "content": prompt}]
            formatted_prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt")
            if self.device == "cuda":
                inputs = inputs.to(self.device)
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.1,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # ë””ì½”ë”©
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
            if formatted_prompt in response:
                response = response.replace(formatted_prompt, "").strip()
            
            return response
            
        except Exception as e:
            return f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def ask_content(self, question):
        """ë‚´ìš© ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ"""
        if not self.model or not self.embedding_model:
            return "âŒ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            print(f"\nğŸ” ë‚´ìš© ê²€ìƒ‰ ì¤‘: {question}")
            
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            context = self._get_relevant_context(question)
            
            if not context:
                return "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ë‚´ìš© í•„ì‚¬ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¤ìŒ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ê·œì¹™:
1. ì°¸ê³  ë¬¸ì„œì— ëª…ì‹œëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ëŠ” ë‚´ìš©ì€ ì¶”ë¡ í•˜ì§€ ë§ˆì„¸ìš”
3. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”"""
            
            response = self._generate_response(prompt)
            return response
            
        except Exception as e:
            return f"âŒ ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def mimic_style(self, content):
        """ë¬¸ì²´ ëª¨ë°© ìƒì„±"""
        if not self.model or not self.embedding_model:
            return "âŒ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            print(f"\nâœï¸ ë¬¸ì²´ ëª¨ë°© ì¤‘: {content[:50]}...")
            
            # ìŠ¤íƒ€ì¼ ì°¸ì¡°ë¥¼ ìœ„í•œ ë¬¸ì„œ ê²€ìƒ‰
            context = self._get_relevant_context(content)
            
            if not context:
                return "âŒ ì°¸ì¡°í•  ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ìŠ¤íƒ€ì¼ ëª¨ë°© í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¤ìŒ ì°¸ê³  ë¬¸ì„œì˜ ë¬¸ì²´ë¥¼ ë¶„ì„í•˜ê³  ëª¨ë°©í•˜ì—¬ ì…ë ¥ ë‚´ìš©ì„ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{context}

ì…ë ¥ ë‚´ìš©: {content}

ì‘ì—… ìˆœì„œ:
1. ì°¸ê³  ë¬¸ì„œì˜ ë¬¸ì²´(ì–´íœ˜, ë¬¸ì¥ êµ¬ì¡°, í†¤ì•¤ë§¤ë„ˆ) ë¶„ì„
2. í•µì‹¬ ìŠ¤íƒ€ì¼ ê·œì¹™ ì •ì˜
3. ì…ë ¥ ë‚´ìš©ì„ í•´ë‹¹ ìŠ¤íƒ€ì¼ë¡œ ì¬ì‘ì„±

ì¬ì‘ì„±ëœ ê²°ê³¼:"""
            
            response = self._generate_response(prompt)
            return response
            
        except Exception as e:
            return f"âŒ ë¬¸ì²´ ëª¨ë°© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    localmind = LocalMindSystem()
    
    if not localmind.model:
        print("ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    print("\n" + "="*60)
    print("ğŸ§  LocalMind - ë¡œì»¬ AI ë¬¸ì„œ ì–´ì‹œìŠ¤í„´íŠ¸")
    print("="*60)
    print("1. ë‚´ìš© ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸")
    print("2. ë¬¸ì²´ ëª¨ë°© í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("\nğŸ“‹ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # 1. ë‚´ìš© ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸
    test_questions = [
        "ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í•µì‹¬ í‚¤ì›Œë“œë‚˜ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ë¬¸ì„œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²°ë¡ ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    print("\nğŸ” ë‚´ìš© ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ í…ŒìŠ¤íŠ¸:")
    for i, question in enumerate(test_questions, 1):
        print(f"\n[í…ŒìŠ¤íŠ¸ {i}] {question}")
        answer = localmind.ask_content(question)
        print(f"[ë‹µë³€] {answer}")
        print("-" * 40)
    
    # 2. ë¬¸ì²´ ëª¨ë°© í…ŒìŠ¤íŠ¸
    test_contents = [
        "ì´ ë³´ê³ ì„œë¥¼ ê°„ê²°í•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ìš”ì•½í•˜ì‹œì˜¤.",
        "ë‹¤ìŒ ë‚´ìš©ì„ í•™ìˆ ì ì¸ ë¬¸ì²´ë¡œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”: ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë°œì „í•˜ê³  ìˆë‹¤.",
        "ë¹„ì¦ˆë‹ˆìŠ¤ ì œì•ˆì„œ í˜•íƒœë¡œ ë‹¤ìŒì„ ì‘ì„±í•´ì£¼ì„¸ìš”: ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´"
    ]
    
    print("\nâœï¸ ë¬¸ì²´ ëª¨ë°© í…ŒìŠ¤íŠ¸:")
    for i, content in enumerate(test_contents, 1):
        print(f"\n[í…ŒìŠ¤íŠ¸ {i}] {content}")
        result = localmind.mimic_style(content)
        print(f"[ê²°ê³¼] {result}")
        print("-" * 40)
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()