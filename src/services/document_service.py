"""
ë¬¸ì„œ ì²˜ë¦¬ ì„œë¹„ìŠ¤ - ê³ ë„í™”ëœ ë¬¸ì„œ ê´€ë¦¬
"""

import os
import hashlib
import time
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import mimetypes
from datetime import datetime

# ë¬¸ì„œ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
from llama_index.core import SimpleDirectoryReader, Document
from llama_index.core.text_splitter import SentenceSplitter
import kss
import faiss
import numpy as np
import pickle

from config.settings import config
from src.services.database_service import db_service
from src.core.ai_engine import get_ai_engine

class DocumentProcessor:
    """ë¬¸ì„œ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.supported_formats = config.processing.supported_formats
        self.max_file_size = config.processing.max_file_size
        self.chunk_size = config.processing.chunk_size
        self.chunk_overlap = config.processing.chunk_overlap
        self.batch_size = config.processing.batch_size
    
    def validate_file(self, filepath: str) -> Tuple[bool, str]:
        """íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
        if not os.path.exists(filepath):
            return False, "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(filepath)
        if file_size > self.max_file_size:
            return False, f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. (ìµœëŒ€: {self.max_file_size // (1024*1024)}MB)"
        
        # íŒŒì¼ í˜•ì‹ í™•ì¸
        file_ext = Path(filepath).suffix.lower()
        if file_ext not in self.supported_formats:
            return False, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(self.supported_formats)}"
        
        return True, "ìœ íš¨í•œ íŒŒì¼ì…ë‹ˆë‹¤."
    
    def calculate_file_hash(self, filepath: str) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def extract_text(self, filepath: str) -> List[Document]:
        """í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # SimpleDirectoryReaderë¡œ ë¬¸ì„œ ë¡œë“œ
            documents = SimpleDirectoryReader(
                input_files=[filepath]
            ).load_data()
            
            return documents
            
        except Exception as e:
            raise Exception(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    def split_text(self, documents: List[Document]) -> List[Dict]:
        """í…ìŠ¤íŠ¸ ë¶„í• """
        chunks = []
        
        for doc in documents:
            # í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬
            sentences = kss.split_sentences(doc.text)
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
            current_chunk = ""
            current_length = 0
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence or len(sentence) < 10:
                    continue
                
                # ì²­í¬ í¬ê¸° í™•ì¸
                if current_length + len(sentence) > self.chunk_size and current_chunk:
                    # í˜„ì¬ ì²­í¬ ì €ì¥
                    chunks.append({
                        'text': current_chunk.strip(),
                        'metadata': {
                            'source': doc.metadata.get('file_name', 'Unknown'),
                            'chunk_id': len(chunks),
                            'length': len(current_chunk.strip())
                        }
                    })
                    
                    # ìƒˆ ì²­í¬ ì‹œì‘ (ì˜¤ë²„ë© ê³ ë ¤)
                    if self.chunk_overlap > 0:
                        overlap_text = current_chunk[-self.chunk_overlap:]
                        current_chunk = overlap_text + " " + sentence
                        current_length = len(current_chunk)
                    else:
                        current_chunk = sentence
                        current_length = len(sentence)
                else:
                    current_chunk += " " + sentence
                    current_length += len(sentence)
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
            if current_chunk.strip():
                chunks.append({
                    'text': current_chunk.strip(),
                    'metadata': {
                        'source': doc.metadata.get('file_name', 'Unknown'),
                        'chunk_id': len(chunks),
                        'length': len(current_chunk.strip())
                    }
                })
        
        return chunks
    
    def create_embeddings(self, chunks: List[Dict]) -> np.ndarray:
        """ì„ë² ë”© ìƒì„±"""
        ai_engine = get_ai_engine()
        
        if not ai_engine.embedding_model:
            raise Exception("ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        embeddings = []
        texts = [chunk['text'] for chunk in chunks]
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]
            batch_embeddings = ai_engine.embedding_model.get_text_embedding_batch(batch_texts)
            embeddings.extend(batch_embeddings)
            
            print(f"   ì„ë² ë”© ì§„í–‰ë¥ : {min(i + self.batch_size, len(texts))}/{len(texts)}")
        
        return np.array(embeddings).astype('float32')

class DocumentService:
    """ë¬¸ì„œ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.processor = DocumentProcessor()
        self.upload_dir = Path("data/uploads")
        self.upload_dir.mkdir(parents=True, exist_ok=True)
    
    def upload_document(self, file_content: bytes, filename: str) -> Dict:
        """ë¬¸ì„œ ì—…ë¡œë“œ"""
        try:
            # íŒŒì¼ ì €ì¥
            filepath = self.upload_dir / filename
            with open(filepath, 'wb') as f:
                f.write(file_content)
            
            # íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
            is_valid, message = self.processor.validate_file(str(filepath))
            if not is_valid:
                os.remove(filepath)
                return {'success': False, 'message': message}
            
            # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
            file_size = len(file_content)
            file_type = mimetypes.guess_type(filename)[0] or 'application/octet-stream'
            file_hash = self.processor.calculate_file_hash(str(filepath))
            
            # ì¤‘ë³µ íŒŒì¼ í™•ì¸
            existing_docs = db_service.get_documents()
            for doc in existing_docs:
                if doc['hash'] == file_hash:
                    os.remove(filepath)
                    return {
                        'success': False, 
                        'message': f"ë™ì¼í•œ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {doc['filename']}"
                    }
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            doc_id = db_service.add_document(
                filename=filename,
                filepath=str(filepath),
                file_type=file_type,
                file_size=file_size,
                file_hash=file_hash
            )
            
            return {
                'success': True,
                'message': f'íŒŒì¼ "{filename}"ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.',
                'document_id': doc_id
            }
            
        except Exception as e:
            return {'success': False, 'message': f'ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}'}
    
    def process_document(self, doc_id: str) -> Dict:
        """ë¬¸ì„œ ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            # ë¬¸ì„œ ì •ë³´ ì¡°íšŒ
            documents = db_service.get_documents()
            doc_info = next((d for d in documents if d['id'] == doc_id), None)
            
            if not doc_info:
                return {'success': False, 'message': 'ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}
            
            if doc_info['processed']:
                return {'success': False, 'message': 'ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œì…ë‹ˆë‹¤.'}
            
            filepath = doc_info['filepath']
            
            print(f"ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {doc_info['filename']}")
            
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            print("1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
            documents = self.processor.extract_text(filepath)
            
            # 2. í…ìŠ¤íŠ¸ ë¶„í• 
            print("2. í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
            chunks = self.processor.split_text(documents)
            
            if not chunks:
                return {'success': False, 'message': 'ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.'}
            
            # 3. ì„ë² ë”© ìƒì„±
            print("3. ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = self.processor.create_embeddings(chunks)
            
            # 4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
            print("4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ ì¤‘...")
            self._update_vector_database(chunks, embeddings)
            
            # 5. ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            db_service.update_document_processed(
                doc_id=doc_id,
                processed=True,
                processing_time=processing_time,
                chunk_count=len(chunks)
            )
            
            # ë¡œê·¸ ê¸°ë¡
            db_service.add_log(
                level='INFO',
                message=f'ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {doc_info["filename"]}',
                module='DocumentService',
                metadata={
                    'document_id': doc_id,
                    'chunk_count': len(chunks),
                    'processing_time': processing_time
                }
            )
            
            print(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ! ({processing_time:.2f}ì´ˆ)")
            
            return {
                'success': True,
                'message': f'ë¬¸ì„œ "{doc_info["filename"]}" ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
                'chunk_count': len(chunks),
                'processing_time': processing_time
            }
            
        except Exception as e:
            # ì˜¤ë¥˜ ë¡œê·¸ ê¸°ë¡
            db_service.add_log(
                level='ERROR',
                message=f'ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}',
                module='DocumentService',
                metadata={'document_id': doc_id}
            )
            
            return {'success': False, 'message': f'ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}'}
    
    def _update_vector_database(self, chunks: List[Dict], embeddings: np.ndarray):
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸"""
        faiss_path = config.database.faiss_index_path
        data_path = config.database.text_data_path
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        existing_texts = []
        existing_metadatas = []
        existing_embeddings = None
        
        if os.path.exists(faiss_path) and os.path.exists(data_path):
            # ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ
            existing_index = faiss.read_index(faiss_path)
            existing_embeddings = np.zeros((existing_index.ntotal, existing_index.d), dtype=np.float32)
            existing_index.reconstruct_n(0, existing_index.ntotal, existing_embeddings)
            
            # ê¸°ì¡´ í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
            with open(data_path, 'rb') as f:
                data = pickle.load(f)
                existing_texts = data['texts']
                existing_metadatas = data['metadatas']
        
        # ìƒˆ ë°ì´í„° ì¶”ê°€
        new_texts = [chunk['text'] for chunk in chunks]
        new_metadatas = [chunk['metadata'] for chunk in chunks]
        
        all_texts = existing_texts + new_texts
        all_metadatas = existing_metadatas + new_metadatas
        
        # ì„ë² ë”© ê²°í•©
        if existing_embeddings is not None:
            all_embeddings = np.vstack([existing_embeddings, embeddings])
        else:
            all_embeddings = embeddings
        
        # ìƒˆ FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = all_embeddings.shape[1]
        index = faiss.IndexFlatIP(dimension)
        index.add(all_embeddings)
        
        # ì €ì¥
        os.makedirs(os.path.dirname(faiss_path), exist_ok=True)
        faiss.write_index(index, faiss_path)
        
        with open(data_path, 'wb') as f:
            pickle.dump({
                'texts': all_texts,
                'metadatas': all_metadatas,
                'embedding_model': config.model.embedding_model
            }, f)
    
    def delete_document(self, doc_id: str) -> Dict:
        """ë¬¸ì„œ ì‚­ì œ"""
        try:
            success = db_service.delete_document(doc_id)
            
            if success:
                # TODO: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œë„ í•´ë‹¹ ë¬¸ì„œì˜ ì²­í¬ë“¤ ì œê±°
                # í˜„ì¬ëŠ” ì „ì²´ ì¬êµ¬ì¶•ì´ í•„ìš”í•¨
                
                db_service.add_log(
                    level='INFO',
                    message=f'ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {doc_id}',
                    module='DocumentService'
                )
                
                return {'success': True, 'message': 'ë¬¸ì„œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.'}
            else:
                return {'success': False, 'message': 'ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}
                
        except Exception as e:
            return {'success': False, 'message': f'ì‚­ì œ ì‹¤íŒ¨: {str(e)}'}
    
    def get_document_stats(self) -> Dict:
        """ë¬¸ì„œ í†µê³„"""
        documents = db_service.get_documents()
        
        total_count = len(documents)
        processed_count = sum(1 for doc in documents if doc['processed'])
        total_size = sum(doc['file_size'] for doc in documents)
        total_chunks = sum(doc['chunk_count'] or 0 for doc in documents)
        
        # íŒŒì¼ í˜•ì‹ë³„ í†µê³„
        format_stats = {}
        for doc in documents:
            ext = Path(doc['filename']).suffix.lower()
            format_stats[ext] = format_stats.get(ext, 0) + 1
        
        return {
            'total_documents': total_count,
            'processed_documents': processed_count,
            'pending_documents': total_count - processed_count,
            'total_size_mb': round(total_size / (1024 * 1024), 2),
            'total_chunks': total_chunks,
            'format_distribution': format_stats
        }
    
    def rebuild_vector_database(self) -> Dict:
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬êµ¬ì¶•"""
        try:
            print("ğŸ”„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬êµ¬ì¶• ì‹œì‘...")
            
            # ì²˜ë¦¬ëœ ë¬¸ì„œë“¤ ì¡°íšŒ
            documents = db_service.get_documents(processed_only=True)
            
            if not documents:
                return {'success': False, 'message': 'ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.'}
            
            all_chunks = []
            all_embeddings = []
            
            for doc in documents:
                print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {doc['filename']}")
                
                # ë¬¸ì„œ ì¬ì²˜ë¦¬
                doc_chunks = self.processor.split_text(
                    self.processor.extract_text(doc['filepath'])
                )
                
                chunk_embeddings = self.processor.create_embeddings(doc_chunks)
                
                all_chunks.extend(doc_chunks)
                all_embeddings.append(chunk_embeddings)
            
            # ì„ë² ë”© ê²°í•©
            combined_embeddings = np.vstack(all_embeddings)
            
            # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
            self._update_vector_database(all_chunks, combined_embeddings)
            
            print("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ!")
            
            return {
                'success': True,
                'message': f'ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¬êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ {len(all_chunks)}ê°œ ì²­í¬)',
                'chunk_count': len(all_chunks)
            }
            
        except Exception as e:
            return {'success': False, 'message': f'ì¬êµ¬ì¶• ì‹¤íŒ¨: {str(e)}'}

# ì „ì—­ ë¬¸ì„œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
document_service = DocumentService()