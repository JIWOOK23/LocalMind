"""
AI ì—”ì§„ - í•µì‹¬ AI ê¸°ëŠ¥ ê´€ë¦¬
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import numpy as np
import faiss
import pickle
import os
from typing import List, Dict, Optional, Tuple
import warnings
from datetime import datetime

from config.settings import config

# ê²½ê³  ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", message=".*encoder_attention_mask.*")

class AIEngine:
    """AI ì—”ì§„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.device = self._get_device()
        self.model = None
        self.tokenizer = None
        self.embedding_model = None
        self.index = None
        self.texts = []
        self.metadatas = []
        self.is_initialized = False
        
        self._initialize()
    
    def _get_device(self) -> str:
        """ë””ë°”ì´ìŠ¤ ê²°ì •"""
        if config.model.device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return config.model.device
    
    def _initialize(self):
        """AI ì—”ì§„ ì´ˆê¸°í™”"""
        try:
            print("ğŸš€ AI ì—”ì§„ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
            
            # LLM ì´ˆê¸°í™”
            self._initialize_llm()
            
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
            self._initialize_embedding()
            
            # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
            self._load_vector_db()
            
            self.is_initialized = True
            print("âœ… AI ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ AI ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _initialize_llm(self):
        """LLM ëª¨ë¸ ì´ˆê¸°í™”"""
        print(f"1. LLM ëª¨ë¸ ë¡œë“œ ì¤‘... ({config.model.llm_model})")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            config.model.llm_model,
            trust_remote_code=True
        )
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            config.model.llm_model,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None,
            trust_remote_code=True
        )
        
        print("   âœ… LLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def _initialize_embedding(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        print(f"2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... ({config.model.embedding_model})")
        
        self.embedding_model = HuggingFaceEmbedding(
            model_name=config.model.embedding_model
        )
        
        print("   âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def _load_vector_db(self):
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ"""
        faiss_path = config.database.faiss_index_path
        data_path = config.database.text_data_path
        
        if not os.path.exists(faiss_path) or not os.path.exists(data_path):
            print("âš ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•œ í›„ create_db.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        print("3. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì¤‘...")
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        self.index = faiss.read_index(faiss_path)
        
        # í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        with open(data_path, "rb") as f:
            data = pickle.load(f)
            self.texts = data["texts"]
            self.metadatas = data["metadatas"]
        
        print(f"   âœ… {len(self.texts)}ê°œì˜ ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
    
    def generate_response(self, prompt: str, **kwargs) -> str:
        """ì‘ë‹µ ìƒì„±"""
        if not self.is_initialized or not self.model:
            return "âŒ AI ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            # íŒŒë¼ë¯¸í„° ì„¤ì •
            max_tokens = kwargs.get('max_tokens', config.model.max_tokens)
            temperature = kwargs.get('temperature', config.model.temperature)
            top_p = kwargs.get('top_p', config.model.top_p)
            
            # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
            messages = [{"role": "user", "content": prompt}]
            formatted_prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt")
            if self.device == "cuda":
                inputs = inputs.to(self.device)
            
            # ìƒì„±
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # ë””ì½”ë”©
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
            if formatted_prompt in response:
                response = response.replace(formatted_prompt, "").strip()
            
            return response
            
        except Exception as e:
            return f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def search_documents(self, query: str, top_k: int = 5) -> List[Dict]:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.embedding_model or not self.index:
            return []
        
        try:
            # ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
            query_embedding = self.embedding_model.get_text_embedding(query)
            query_vector = np.array([query_embedding]).astype('float32')
            
            # FAISSë¡œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
            scores, indices = self.index.search(query_vector, top_k)
            
            # ê²°ê³¼ ìˆ˜ì§‘
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.texts) and score > 0.1:
                    results.append({
                        'text': self.texts[idx],
                        'metadata': self.metadatas[idx],
                        'score': float(score)
                    })
            
            return results
            
        except Exception as e:
            print(f"ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def ask_content(self, question: str) -> str:
        """ë‚´ìš© ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ"""
        if not self.is_initialized:
            return "âŒ AI ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            search_results = self.search_documents(question)
            
            if not search_results:
                return "âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            for result in search_results:
                source = result['metadata'].get('source', 'Unknown')
                text = result['text']
                context_parts.append(f"[ì¶œì²˜: {source}] {text}")
            
            context = "\n\n".join(context_parts)
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ë‹¤ìŒ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ê·œì¹™:
1. ì°¸ê³  ë¬¸ì„œì— ëª…ì‹œëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ëŠ” ë‚´ìš©ì€ ì¶”ë¡ í•˜ì§€ ë§ˆì„¸ìš”
3. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
4. ê°€ëŠ¥í•˜ë©´ ì¶œì²˜ë¥¼ ì–¸ê¸‰í•´ì£¼ì„¸ìš”

ë‹µë³€:"""
            
            return self.generate_response(prompt)
            
        except Exception as e:
            return f"âŒ ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def mimic_style(self, content: str) -> str:
        """ë¬¸ì²´ ëª¨ë°© ìƒì„±"""
        if not self.is_initialized:
            return "âŒ AI ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            # ìŠ¤íƒ€ì¼ ì°¸ì¡°ë¥¼ ìœ„í•œ ë¬¸ì„œ ê²€ìƒ‰
            search_results = self.search_documents(content)
            
            if not search_results:
                return "âŒ ì°¸ì¡°í•  ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            for result in search_results:
                context_parts.append(result['text'])
            
            context = "\n\n".join(context_parts)
            
            # ìŠ¤íƒ€ì¼ ëª¨ë°© í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¤ìŒ ì°¸ê³  ë¬¸ì„œì˜ ë¬¸ì²´ë¥¼ ë¶„ì„í•˜ê³  ëª¨ë°©í•˜ì—¬ ì…ë ¥ ë‚´ìš©ì„ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{context}

ì…ë ¥ ë‚´ìš©: {content}

ì‘ì—… ìˆœì„œ:
1. ì°¸ê³  ë¬¸ì„œì˜ ë¬¸ì²´(ì–´íœ˜, ë¬¸ì¥ êµ¬ì¡°, í†¤ì•¤ë§¤ë„ˆ) ë¶„ì„
2. í•µì‹¬ ìŠ¤íƒ€ì¼ ê·œì¹™ ì •ì˜
3. ì…ë ¥ ë‚´ìš©ì„ í•´ë‹¹ ìŠ¤íƒ€ì¼ë¡œ ì¬ì‘ì„±

ì¬ì‘ì„±ëœ ê²°ê³¼:"""
            
            return self.generate_response(prompt)
            
        except Exception as e:
            return f"âŒ ë¬¸ì²´ ëª¨ë°© ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def get_system_info(self) -> Dict:
        """ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        return {
            'is_initialized': self.is_initialized,
            'device': self.device,
            'model_name': config.model.llm_model,
            'embedding_model': config.model.embedding_model,
            'document_count': len(self.texts),
            'has_vector_db': self.index is not None
        }

# ì „ì—­ AI ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
ai_engine = None

def get_ai_engine() -> AIEngine:
    """AI ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global ai_engine
    if ai_engine is None:
        ai_engine = AIEngine()
    return ai_engine